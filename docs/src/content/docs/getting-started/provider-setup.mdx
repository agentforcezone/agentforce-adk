---
title: Provider Setup
description: Configure AI providers (Ollama, OpenRouter) to power your AgentForce ADK agents
---

import { Badge, Aside, Tabs, TabItem, Card, CardGrid } from '@astrojs/starlight/components';

# Provider Setup Guide

<br />

<Badge text="Essential" variant="caution" /> <Badge text="Multiple Options" variant="note" />

<br />
<br />

AgentForce ADK supports multiple AI providers, giving you flexibility to choose between local and cloud-based models based on your needs.

## Available Providers

<CardGrid>
  <Card title="Ollama" icon="laptop">
    <Badge text="Local" variant="tip" /> <Badge text="Free" variant="note" /> <Badge text="Privacy" variant="caution" />
    
    **Best for:**
    - Development and testing
    - Privacy-sensitive applications
    - Offline environments
    - Cost-effective solutions
  </Card>
  
  <Card title="OpenRouter" icon="external">
    <Badge text="Cloud" variant="tip" /> <Badge text="Multiple Providers" variant="note" /> <Badge text="API Key Required" variant="caution" />
    
    **Best for:**
    - Production applications
    - Access to latest models
    - Scalable solutions
    - Multiple model providers
  </Card>
</CardGrid>

<Aside type="note">
  **Coming Soon**: Native OpenAI, Anthropic, and Google provider support. Currently available through OpenRouter.
</Aside>

## Ollama Setup (Local Models)

Ollama allows you to run large language models locally on your machine.

### Installation

<Tabs>
  <TabItem label="macOS/Linux">
    ```bash
    # Install Ollama
    curl -fsSL https://ollama.ai/install.sh | sh
    
    # Verify installation
    ollama --version
    ```
  </TabItem>
  
  <TabItem label="Windows">
    ```powershell
    # Download and install from https://ollama.ai/download
    # Or use Windows Subsystem for Linux (WSL)
    wsl curl -fsSL https://ollama.ai/install.sh | sh
    ```
  </TabItem>
  
  <TabItem label="Docker">
    ```bash
    # Run Ollama in Docker
    docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
    
    # Pull a model
    docker exec -it ollama ollama pull gemma3:12b
    ```
  </TabItem>
</Tabs>

### Recommended Models

Pull models based on your hardware and use case:

<Tabs>
  <TabItem label="Lightweight (4-8GB RAM)">
    ```bash
    # Fast and efficient models
    ollama pull phi4-mini:latest        # 3.8B params, great for development
    ollama pull gemma3:4b              # 4B params, balanced performance
    ollama pull llama3.2:3b            # 3B params, Meta's model
    ```
  </TabItem>
  
  <TabItem label="Balanced (8-16GB RAM)">
    ```bash
    # Good performance and capability
    ollama pull gemma3:12b             # 12B params, recommended
    ollama pull llama3.2:7b            # 7B params, versatile
    ollama pull phi4:latest            # Microsoft's latest model
    ```
  </TabItem>
  
  <TabItem label="High-End (16GB+ RAM)">
    ```bash
    # Maximum capability
    ollama pull llama3.1:70b           # 70B params, excellent performance
    ollama pull mixtral:8x7b           # Mixture of experts model
    ollama pull codellama:34b          # Specialized for code generation
    ```
  </TabItem>
</Tabs>

### Verify Ollama Setup

Test your Ollama installation:

```bash
# Check if Ollama is running
ollama list

# Test a model
ollama run gemma3:12b "Hello, how are you?"

# Check Ollama API
curl http://localhost:11434/api/tags
```

### Using Ollama in AgentForce ADK

```typescript
import { AgentForceAgent } from '@agentforce/adk';

const agent = new AgentForceAgent({
  name: "LocalAgent",
  type: "ollama-agent"
})
  .useLLM("ollama", "gemma3:12b")  // provider: "ollama", model: "gemma3:12b"
  .systemPrompt("You are a helpful assistant running locally")
  .prompt("What are the benefits of local AI models?");

await agent.run();
const response = await agent.output("text");
console.log(response);
```

## OpenRouter Setup (Cloud Models)

OpenRouter provides access to multiple AI providers through a single API.

### Get API Key

1. Visit [OpenRouter.ai](https://openrouter.ai/)
2. Sign up for a free account
3. Navigate to [API Keys](https://openrouter.ai/keys)
4. Create a new API key

### Configure API Key

<Tabs>
  <TabItem label="Environment Variable">
    ```bash
    # Add to your shell profile (.bashrc, .zshrc, etc.)
    export OPENROUTER_API_KEY="sk-or-v1-your-api-key-here"
    
    # Or add to .env file
    echo "OPENROUTER_API_KEY=sk-or-v1-your-api-key-here" >> .env
    ```
  </TabItem>
  
  <TabItem label="Runtime Configuration">
    ```typescript
    // Set at runtime (not recommended for production)
    process.env.OPENROUTER_API_KEY = "sk-or-v1-your-api-key-here";
    
    import { AgentForceAgent } from '@agentforce/adk';
    // ... rest of your code
    ```
  </TabItem>
</Tabs>

### Available Models

OpenRouter provides access to models from multiple providers:

<CardGrid>
  <Card title="Free Models" icon="laptop">
    ```typescript
    // Great for development and testing
    .useLLM("openrouter", "moonshotai/kimi-k2:free")
    .useLLM("openrouter", "anthropic/claude-3-haiku:beta")
    .useLLM("openrouter", "google/gemma-2-9b-it:free")
    ```
  </Card>
  
  <Card title="OpenAI Models" icon="external">
    ```typescript
    // OpenAI's latest models
    .useLLM("openrouter", "openai/gpt-4")
    .useLLM("openrouter", "openai/gpt-4-turbo")
    .useLLM("openrouter", "openai/gpt-3.5-turbo")
    ```
  </Card>
  
  <Card title="Anthropic Models" icon="puzzle">
    ```typescript
    // Claude models
    .useLLM("openrouter", "anthropic/claude-3-opus")
    .useLLM("openrouter", "anthropic/claude-3-sonnet")
    .useLLM("openrouter", "anthropic/claude-3-haiku")
    ```
  </Card>
  
  <Card title="Meta Models" icon="rocket">
    ```typescript
    // Llama models
    .useLLM("openrouter", "meta-llama/llama-3.1-8b-instruct")
    .useLLM("openrouter", "meta-llama/llama-3.1-70b-instruct")
    .useLLM("openrouter", "meta-llama/llama-3.1-405b-instruct")
    ```
  </Card>
</CardGrid>

### Using OpenRouter in AgentForce ADK

```typescript
import { AgentForceAgent } from '@agentforce/adk';

const agent = new AgentForceAgent({
  name: "CloudAgent",
  type: "openrouter-agent"
})
  .useLLM("openrouter", "openai/gpt-4")  // provider: "openrouter", model: "openai/gpt-4"
  .systemPrompt("You are a cloud-powered AI assistant")
  .prompt("What are the advantages of cloud-based AI models?");

await agent.run();
const response = await agent.output("text");
console.log(response);
```

### Cost Management

Monitor your OpenRouter usage:

1. Check your [OpenRouter Dashboard](https://openrouter.ai/activity)
2. Set up budget alerts
3. Use free models for development
4. Consider model costs for production

<Aside type="tip">
  **Pro Tip**: Start with free models during development, then upgrade to premium models for production use.
</Aside>

## Provider Comparison

| Feature | Ollama (Local) | OpenRouter (Cloud) |
|---------|----------------|-------------------|
| **Cost** | Free | Pay-per-use |
| **Privacy** | Complete | Provider-dependent |
| **Latency** | Low (local) | Network-dependent |
| **Models** | Limited selection | 100+ models |
| **Setup** | Installation required | API key only |
| **Offline** | ✅ Yes | ❌ No |
| **Scalability** | Hardware-limited | Unlimited |

## Switching Between Providers

AgentForce ADK makes it easy to switch providers:

```typescript
import { AgentForceAgent } from '@agentforce/adk';

const config = {
  name: "FlexibleAgent",
  type: "multi-provider-agent"
};

// Use local model for development
const devAgent = new AgentForceAgent(config)
  .useLLM("ollama", "gemma3:12b")
  .systemPrompt("Development mode")
  .prompt("Test prompt");

// Use cloud model for production
const prodAgent = new AgentForceAgent(config)
  .useLLM("openrouter", "openai/gpt-4")
  .systemPrompt("Production mode")
  .prompt("Production prompt");

// Dynamic provider selection
const useCloud = process.env.NODE_ENV === 'production';
const dynamicAgent = new AgentForceAgent(config)
  .useLLM(
    useCloud ? "openrouter" : "ollama",
    useCloud ? "openai/gpt-4" : "gemma3:12b"
  )
  .systemPrompt("Dynamic provider selection")
  .prompt("Adaptive prompt");
```

## Troubleshooting

### Ollama Issues

**Ollama not starting:**
```bash
# Check if Ollama is running
ps aux | grep ollama

# Start Ollama manually
ollama serve

# Check logs
ollama logs
```

**Model not found:**
```bash
# List available models
ollama list

# Pull missing model
ollama pull gemma3:12b

# Check model info
ollama show gemma3:12b
```

**Connection refused:**
```bash
# Check if Ollama API is accessible
curl http://localhost:11434/api/tags

# Restart Ollama service
brew services restart ollama  # macOS
sudo systemctl restart ollama  # Linux
```

### OpenRouter Issues

**API key not working:**
- Verify key at [OpenRouter Keys](https://openrouter.ai/keys)
- Check environment variable: `echo $OPENROUTER_API_KEY`
- Ensure key starts with `sk-or-v1-`

**Rate limiting:**
- Check your usage at [OpenRouter Activity](https://openrouter.ai/activity)
- Consider upgrading your plan
- Use free models for development

**Model not available:**
- Check [OpenRouter Models](https://openrouter.ai/models) for availability
- Some models require approval or higher tier plans

## Next Steps

With your providers configured, you're ready to:

<CardGrid>
  <Card title="Create Your First Agent" icon="rocket">
    Build a simple agent with your configured provider
    <a href="/getting-started/quick-start">→ Quick Start</a>
  </Card>
  
  <Card title="Deploy as Server" icon="external">
    Learn how to deploy agents as HTTP APIs
    <a href="/getting-started/server-mode">→ Server Mode</a>
  </Card>
  
  <Card title="Explore Multiple Providers" icon="puzzle">
    Learn advanced provider usage patterns
    <a href="/guides/providers">→ Provider Guide</a>
  </Card>
  
  <Card title="View Examples" icon="document">
    See real-world provider implementations
    <a href="/examples/basic">→ Examples</a>
  </Card>
</CardGrid>

Great! You now have AI providers configured and ready to power your AgentForce ADK agents. Choose Ollama for local development and privacy, or OpenRouter for access to the latest cloud models.
