---
title: Server Mode
description: Deploy AgentForce ADK agents as HTTP APIs with built-in server functionality
---

import { Badge, Aside, Tabs, TabItem, Card, CardGrid } from '@astrojs/starlight/components';

# Server Mode Guide

<br />

<Badge text="Production Ready" variant="tip" /> <Badge text="OpenAI Compatible" variant="note" /> <Badge text="HTTP APIs" variant="caution" />

<br />
<br />

AgentForce ADK includes powerful built-in server capabilities, allowing you to deploy your agents as HTTP APIs with minimal configuration.

## Quick Server Setup

Transform any agent into a web service in just a few lines:

```typescript
import { AgentForceAgent } from '@agentforce/adk';

// Create your agent
const agent = new AgentForceAgent({
  name: "WebAgent",
  type: "web-service"
})
  .useLLM("ollama", "gemma3:12b")
  .systemPrompt("You are a helpful web API assistant");

// Deploy as HTTP server
await agent.serve("localhost", 3000);
console.log("ðŸš€ Agent server running at http://localhost:3000");
```

## AgentForceServer Class

For more advanced server functionality, use the dedicated `AgentForceServer` class:

```typescript
import { AgentForceServer, AgentForceAgent, type ServerConfig } from '@agentforce/adk';

// Configure server
const serverConfig: ServerConfig = {
  name: "ProductionAPI",
  logger: "json"  // or "pretty" for development
};

// Create multiple agents
const chatAgent = new AgentForceAgent({
  name: "ChatAgent",
  type: "conversational"
})
  .useLLM("ollama", "gemma3:12b")
  .systemPrompt("You are a friendly chat assistant");

const codeAgent = new AgentForceAgent({
  name: "CodeAgent", 
  type: "code-assistant"
})
  .useLLM("openrouter", "openai/gpt-4")
  .systemPrompt("You are a code review and generation expert");

// Create server with multiple endpoints
const server = new AgentForceServer(serverConfig)
  .addRouteAgent("POST", "/chat", chatAgent)
  .addRouteAgent("POST", "/code", codeAgent)
  .addRoute("GET", "/health", { status: "ok" })
  .useOpenAICompatibleRouting(chatAgent);

// Start server
await server.serve("0.0.0.0", 8080);
```

## Server Features

<CardGrid>
  <Card title="OpenAI Compatibility" icon="external">
    Full OpenAI chat completions API compatibility for seamless integration with existing tools.
  </Card>
  
  <Card title="Multiple Endpoints" icon="list-format">
    Host multiple agents on different routes with custom validation schemas.
  </Card>
  
  <Card title="Static Routes" icon="document">
    Add utility endpoints for health checks, documentation, and webhooks.
  </Card>
  
  <Card title="Structured Logging" icon="setting">
    Built-in logging with Pino for production monitoring and debugging.
  </Card>
</CardGrid>

## API Endpoints

### Agent Routes

Agent routes process AI requests and return intelligent responses:

<Tabs>
  <TabItem label="Basic Agent Route">
    ```typescript
    // Add agent to handle POST requests
    server.addRouteAgent("POST", "/story", storyAgent);
    
    // Usage
    curl -X POST http://localhost:3000/story \
      -H "Content-Type: application/json" \
      -d '{"prompt": "Write a short story about AI"}'
    ```
  </TabItem>
  
  <TabItem label="GET Agent Route">
    ```typescript
    // Add agent to handle GET requests with query parameters
    server.addRouteAgent("GET", "/ask", questionAgent);
    
    // Usage
    curl "http://localhost:3000/ask?prompt=What%20is%20TypeScript"
    ```
  </TabItem>
  
  <TabItem label="Schema Validation">
    ```typescript
    import type { RouteAgentSchema } from '@agentforce/adk';

    // Define validation schema
    const taskSchema: RouteAgentSchema = {
      input: ["prompt", "priority", "category"],
      output: ["success", "prompt", "response", "priority", "category", "timestamp"]
    };

    // Add route with strict validation
    server.addRouteAgent("POST", "/task", taskAgent, taskSchema);
    
    // Only accepts requests with exactly these fields
    ```
  </TabItem>
</Tabs>

### Static Routes

Static routes return predefined data without AI processing:

```typescript
server
  // Health check endpoint
  .addRoute("GET", "/health", { status: "ok", timestamp: new Date() })
  
  // API information
  .addRoute("GET", "/info", { 
    name: "AgentAPI", 
    version: "1.0.0",
    endpoints: ["/chat", "/code", "/health"]
  })
  
  // Dynamic webhook handler
  .addRoute("POST", "/webhook", (context) => ({
    received: true,
    method: context.req.method,
    timestamp: new Date().toISOString()
  }));
```

### OpenAI Compatible Routes

Enable drop-in compatibility with OpenAI tools and SDKs:

```typescript
// Add OpenAI chat completions endpoint
server.useOpenAICompatibleRouting(agent);

// Creates: POST /v1/chat/completions
// Compatible with OpenAI SDK, LangChain, and other tools
```

**OpenAI API Usage:**
```bash
curl -X POST http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ollama/gemma3:12b",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

**Response Format:**
```json
{
  "id": "chatcmpl-1234567890",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "ollama/gemma3:12b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 2,
    "completion_tokens": 9,
    "total_tokens": 11
  }
}
```

## Schema Validation

Enforce strict input validation and structured output formatting:

<Tabs>
  <TabItem label="Define Schema">
    ```typescript
    import type { RouteAgentSchema } from '@agentforce/adk';

    const projectSchema: RouteAgentSchema = {
      input: ["prompt", "project_name", "priority", "deadline"],
      output: ["success", "prompt", "response", "project_name", "priority", "deadline", "created_at"]
    };
    ```
  </TabItem>
  
  <TabItem label="Apply Schema">
    ```typescript
    server.addRouteAgent("POST", "/project", projectAgent, projectSchema);
    ```
  </TabItem>
  
  <TabItem label="Valid Request">
    ```bash
    curl -X POST http://localhost:3000/project \
      -H "Content-Type: application/json" \
      -d '{
        "prompt": "Create a project plan",
        "project_name": "WebApp",
        "priority": "high",
        "deadline": "2025-08-01"
      }'
    ```
  </TabItem>
  
  <TabItem label="Schema Response">
    ```json
    {
      "success": true,
      "prompt": "Create a project plan",
      "response": "Here's a comprehensive project plan...",
      "project_name": "WebApp",
      "priority": "high",
      "deadline": "2025-08-01",
      "created_at": "2025-07-18T10:30:00.000Z"
    }
    ```
  </TabItem>
</Tabs>

**Schema Benefits:**
- **Input Validation**: Reject requests missing required fields
- **Output Consistency**: Standardized response format
- **API Documentation**: Schema serves as endpoint specification
- **Error Prevention**: Clear error messages for invalid requests

## Production Deployment

### Environment Configuration

<Tabs>
  <TabItem label="Development">
    ```typescript
    const server = new AgentForceServer({
      name: "DevServer",
      logger: "pretty"  // Colored console output
    });

    await server.serve("localhost", 3000);
    ```
  </TabItem>
  
  <TabItem label="Production">
    ```typescript
    const server = new AgentForceServer({
      name: "ProductionAPI",
      logger: "json"  // Structured JSON logs
    });

    const port = process.env.PORT || 8080;
    const host = process.env.HOST || "0.0.0.0";
    
    await server.serve(host, port);
    ```
  </TabItem>
  
  <TabItem label="Docker">
    ```dockerfile
    FROM oven/bun:1-alpine
    WORKDIR /app
    COPY package.json bun.lock ./
    RUN bun install --frozen-lockfile
    COPY . .
    EXPOSE 8080
    CMD ["bun", "run", "src/server.ts"]
    ```
  </TabItem>
</Tabs>

### Environment Variables

```bash
# .env file
NODE_ENV=production
PORT=8080
HOST=0.0.0.0

# AI Provider Configuration
OPENROUTER_API_KEY=sk-or-v1-your-key
OLLAMA_HOST=http://localhost:11434

# Logging
LOG_LEVEL=info
```

### Health Monitoring

Add comprehensive health and monitoring endpoints:

```typescript
const server = new AgentForceServer(config)
  // Health check
  .addRoute("GET", "/health", () => ({
    status: "healthy",
    timestamp: new Date().toISOString(),
    uptime: process.uptime()
  }))
  
  // Readiness check
  .addRoute("GET", "/ready", async () => {
    try {
      // Check AI provider connectivity
      const testAgent = new AgentForceAgent({ name: "test", type: "health" })
        .useLLM("ollama", "gemma3:12b");
      
      return { status: "ready", providers: ["ollama"] };
    } catch (error) {
      return { status: "not ready", error: error.message };
    }
  })
  
  // Metrics endpoint
  .addRoute("GET", "/metrics", () => ({
    requests_total: requestCounter,
    response_time_ms: averageResponseTime,
    active_connections: activeConnections
  }));
```

## Integration Examples

### With OpenAI SDK

```typescript
import OpenAI from 'openai';

// Point OpenAI SDK to your AgentForce server
const client = new OpenAI({
  baseURL: 'http://localhost:3000',
  apiKey: 'not-needed',
});

const completion = await client.chat.completions.create({
  model: 'ollama/gemma3:12b',
  messages: [{ role: 'user', content: 'Hello!' }],
});

console.log(completion.choices[0].message.content);
```

### With LangChain

```typescript
import { ChatOpenAI } from '@langchain/openai';

const model = new ChatOpenAI({
  openAIApiKey: 'not-needed',
  configuration: {
    baseURL: 'http://localhost:3000',
  },
});

const response = await model.invoke('Tell me a joke');
console.log(response.content);
```

### Load Balancing

Deploy multiple server instances behind a load balancer:

```yaml
# docker-compose.yml
version: '3.8'
services:
  agent-api-1:
    build: .
    environment:
      - PORT=8080
  
  agent-api-2:
    build: .
    environment:
      - PORT=8080
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - agent-api-1
      - agent-api-2
```

## Error Handling

The server provides comprehensive error handling:

```typescript
// Automatic error responses for invalid requests
{
  "error": "Missing required fields",
  "message": "The following required fields are missing: priority",
  "required": ["prompt", "project_name", "priority"],
  "received": ["prompt", "project_name"]
}

// OpenAI-compatible error format
{
  "error": "Invalid OpenAI chat completion format",
  "message": "Missing or invalid \"model\" field",
  "example": {
    "model": "ollama/gemma3:12b",
    "messages": [{"role": "user", "content": "Hello"}]
  }
}
```

## Performance Optimization

### Concurrent Requests

AgentForce servers handle multiple requests concurrently:

```typescript
// Server automatically handles concurrent requests
// No additional configuration needed
const server = new AgentForceServer(config)
  .addRouteAgent("POST", "/chat", chatAgent);

// Each request is processed independently
await server.serve("0.0.0.0", 8080);
```

### Response Caching

Implement caching for frequently requested content:

```typescript
const cache = new Map();

server.addRoute("GET", "/cached-response", (context) => {
  const key = context.req.url;
  
  if (cache.has(key)) {
    return cache.get(key);
  }
  
  const response = generateExpensiveResponse();
  cache.set(key, response);
  
  return response;
});
```

## Security Considerations

<Aside type="caution">
  **Security Note**: Implement proper authentication and rate limiting for production deployments.
</Aside>

### Basic Security Headers

```typescript
// Add security middleware (implementation depends on your setup)
server.addRoute("*", "/*", (context, next) => {
  context.res.headers.set('X-Content-Type-Options', 'nosniff');
  context.res.headers.set('X-Frame-Options', 'DENY');
  context.res.headers.set('X-XSS-Protection', '1; mode=block');
  return next();
});
```

### Rate Limiting

Consider implementing rate limiting for production use:

```typescript
// Example rate limiting (implement based on your needs)
const rateLimiter = new Map();

server.addRoute("*", "/*", (context, next) => {
  const clientIP = context.req.headers.get('x-forwarded-for') || 'unknown';
  const requests = rateLimiter.get(clientIP) || 0;
  
  if (requests > 100) {  // 100 requests per time window
    return { error: "Rate limit exceeded" };
  }
  
  rateLimiter.set(clientIP, requests + 1);
  return next();
});
```

## Next Steps

<CardGrid>
  <Card title="API Reference" icon="open-book">
    Explore the complete server API documentation
    <a href="/reference/">â†’ API Docs</a>
  </Card>
  
  <Card title="Server Examples" icon="rocket">
    See real-world server deployment examples
    <a href="/examples/server">â†’ Server Examples</a>
  </Card>
  
  <Card title="OpenAI Compatibility" icon="external">
    Learn more about OpenAI API compatibility
    <a href="/guides/openai-compatibility">â†’ OpenAI Guide</a>
  </Card>
  
  <Card title="Production Deployment" icon="setting">
    Advanced deployment patterns and best practices
    <a href="/examples/advanced">â†’ Advanced Examples</a>
  </Card>
</CardGrid>

You now have the knowledge to deploy AgentForce ADK agents as production-ready HTTP APIs with OpenAI compatibility, schema validation, and comprehensive monitoring capabilities!
